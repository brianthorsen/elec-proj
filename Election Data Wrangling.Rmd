---
title: "Election Data Wrangling"
author: "Kean Amidi-Abraham, Ryan Haeri, Kevin Marroquin, Ryan Saraie, Brian Thorsen"
date: "11/27/2016"
output: html_document
---

```{r packages}
library(XML)
library(plyr)
```

## Introduction

In this report, we seek to obtain information from a variety of sources which pertain to voting and demographic information at the county level in the United States. Our goal is to obtain features which can be used to model and develop predictors for the 2016 election results for each U.S. county, and assess how successful our predictors are.

To accomplish this task, we will collect the following information:

- Popular vote from the 2016, 2012, 2008, and 2004 elections

- Various statistics from the 2010 Census data

- Geographic information on each county


These features enable us to develop metrics of similarity between counties. For instance, a nearest-neighbor classifier can look at not only geographical proximity, but similarities in workforce composition, median household income, and voting tendencies in prior elections in order to predict a county's vote in the 2016 election.

Data in this project is available in a variety of formats. Each data source, therefore, must be prepared and examined individually before combination into a final product.

## 2016 Results
#### Credit: Thorsen

```{r 2016 results, eval = FALSE}
results2016 <- read.csv("http://www.stat.berkeley.edu/users/nolan/data/voteProject/2016_US_County_Level_Presidential_Results.csv")

results2016$state_abbr <- as.character(results2016$state_abbr)
results2016$county_name <- as.character(results2016$county_name)
```

2016 data available in CSV directly from the web. State abbreviations and county names are converted from factor to character so as to prevent possible issues in merging data frames later on.

```{r load 2016, echo = FALSE}
load("data/results2016.rda")
```

```{r}
c(length(unique(results2016$state_abbr)), length(unique(results2016$combined_fips)))
nrow(results2016)
```

All 51 states (including D.C.) are present in the 2016 results. 3141 unique counties are represented in the 2016 data, as shown by the number of unique FIPS codes. To join this data frame to the 2012 results, we will merge by the FIPS code for each county.

## 2012 Results
#### Credit: Haeri, Thorsen

2012 election results are available as a set of 50 XML files. From these files, we will extract the votes for Obama, votes for Romney, and county and state information for each county in each state.

```{r list of names}
stateNames <- read.csv("http://www.stat.berkeley.edu/~nolan/data/voteProject/countyVotes2012/stateNames.txt")

stateNamesNoAK <- stateNames[stateNames != "alaska",]
```

Names of all states, including D.C., available from Prof. Nolan's website. For the 2012 election results, Alaska information is not available in an XML file, and must be excluded when obtaining data.

Because each file corresponds to a specific state, it is necessary to apply a function over the contents of `stateNamesNoAK`. The following code stores the results for each state as an element in a list of 50 elements.

#### Utility functions

```{r utility functions}
numCoercer <- function(string) {
  ## Drops whitespace and commas from string of a number and converts remaining digits to numeric.
  string <- gsub('[[:blank:]]|[[:punct:]]', "", string)
  return(as.numeric(string))
}

getCountyName <- function(string) {
  return(gsub("[[:blank:]][[:digit:]].*", "", string))
}

getFipsCode <- function(list) {
  if(typeof(list) == "character") {
    return(as.numeric(gsub("county", "", list)))
  }
  
  sapply(list, function(elem) {
    return(as.numeric(gsub("county", "", elem[["id"]])))
  })
}
```

These functions will be needed later on for transforming the text within the XML files. For instance, the popular vote for Romney in Autagua county is stored as `   17,366` (with whitespace). `numCoercer` should convert this string to the number `17366`. Additionally, the county information obtained using our xPath (`countyPath`, seen below) returns the county as well as the percentage of vote reporting; e.g. `Autauga 100.0% Reporting`. 

`getCountyName` removes the space before digits and everything following a digit, so that it would return `Autauga`. This function has its pitfalls, namely if a county name includes a number. However, this is not a concern for this particular data set, as all counties have purely alphabetic names\*.

`getFipsCode` works somewhat differently, as the FIPS code is stored as an attribute name within the XML file. As such, `xmlAttrs` is used to find its information, and `xmlAttrs` will return a list of vectors when multiple attributes exist for a node. `getFipsCode` looks for the `id` within each element of the list, and coerces it into the proper type. For instance, Autauga county is stored as `"county1001"`, which is converted into `1001`. There is additionally a special case for DC, as there is only one county in DC. `xmlAttrs` only returns a single character vector for DC, so this is accounted for in the `if` clause.

\*(https://en.wikipedia.org/wiki/List_of_United_States_counties_and_county_equivalents)


```{r 2012 results, eval = FALSE}

## Step 1:
## xPaths needed to extract information
demVotePath <- "//*[abbr='Dem']/parent::tr/td[@class='results-popular']"
gopVotePath <- "//*[abbr='GOP']/parent::tr/td[@class='results-popular']"
countyPath <-  "//tbody/tr//th[@class='results-county']"
fipsPath <- "//tbody"

results2012 <- lapply(stateNamesNoAK, function(sName) {

  ## Step 2:
  ## Prepare XML file for each state
  url <- paste("http://www.stat.berkeley.edu/users/nolan/data/voteProject/countyVotes2012/",
               sName,".xml",sep = "")
  stateXML <- xmlParse(url)
  stateRoot <- xmlRoot(stateXML)
  
  ## Step 3:   
  ## Obtain & coerce information into needed data types
  demNumVotes <- numCoercer(xpathSApply(stateRoot, demVotePath, xmlValue))
  gopNumVotes <- numCoercer(xpathSApply(stateRoot, gopVotePath, xmlValue))
  countyNames <- getCountyName(xpathSApply(stateRoot, countyPath, xmlValue))
  countyCodes <- getFipsCode(xpathSApply(stateRoot, fipsPath, xmlAttrs))

  ## Step 4:
  ## Construct data frames to be stored in list
  return(data.frame(state = rep(sName, length(countyNames)),
                    county = countyNames,
                    countyFIPS = countyCodes,
                    obamaVotes2012 = demNumVotes,
                    romneyVotes2012 = gopNumVotes))
})

```

*Step 1*: Voting results for both parties and county names can be found using the given xPaths. The first two find any node with an `abbr` value for the desired party, and move to the parent node directly above. One of its child nodes (a sibling of the `abbr` node) contains the popular vote count: the node whose attribute `class` is named `results-popular`. `fipsPath` moves to the parent node corresponding to each county, because the FIPS code is stored as an attribute name at this node.

*Step 2*: A URL for a given state is formed, and the XML contained within the URL is parsed. Its root will be used for successive functions.

`countyPath` directly locates the county name of each county, with some extra information as noted above.

The FIPS code of each county is stored as the name of an attribute in each `tbody`.

*Step 3*: Using the xPaths in step 2, the information for each county is stored across three vectors through an `xPathSApply` that is then converted into the proper format using the utility functions described above.

*Step 4*: A data frame is constructed containing all necessary information for a given state. The state name is recycled for each county using `rep`. The index order of the three vectors from step 3 keeps each county and vote count properly associated with one another.


```{r reduce2012, eval = FALSE}
results2012 <- Reduce(rbind, results2012[-1], init = results2012[[1]])
results2012$county <- as.character(results2012$county)
results2012$state <- as.character(results2012$state)
```

`results2012` is initially a list of 50 different data frames. We combine the results using a reduce function: all the other DFs are recursively bound to the first element of the list using `Reduce` and `rbind`. Additionally, county and state names are intially stored as factors, so we convert them to characters to prevent possible issues in merging operations.

```{r load 2012 data, echo = FALSE}
load("data/results2012.rda")
elecData <- results2012
```

```{r}
c(length(unique(results2012$state)), length(unique(results2012$countyFIPS)))
nrow(results2012) - nrow(results2016)
```

Each county in `results2012` has a unique FIPS code, just as expected. Because Alaska is missing, there are fewer counties in the 2012 data frame, but Alaska has only 19 boroughs (https://en.wikipedia.org/wiki/List_of_boroughs_and_census_areas_in_Alaska). As such, there should be 9 additional counties that are missing from the 2012 election data. Let's find them.

```{r}
results2016[!(results2016$combined_fips %in% results2012$countyFIPS),c("county_name", "combined_fips")]
```

All of the missing counties appear to be in Alaska, which resolves that particular issue. However, there are 29 county codes in the 2016 data frame that are not in the 2012 data frame; 1 more than the difference in the number of rows. This suggests that there might be a repeated county in the 2012 data frame.

## Merging 2012 and 2016
#### Credit: Thorsen

```{r merge 12 and 16, eval = FALSE}
elecData <- merge(x = results2012, y = results2016,
                        by.x = "countyFIPS", by.y = "combined_fips",
                        all.x = FALSE, all.y = FALSE)
```

```{r, echo = FALSE}
load("data/results12and16.rda")
elecData <- results12and16
```

We merge the 2012 and 2016 data frames by FIPS codes, which allows us to circumvent issues of how state and county names are stored in each data frame. Because the 2012 voting data is, qualitatively speaking, a crucial predictor of the 2016 election results, we do not want to work with counties which are missing this important data point. As such, we use `all.(x/y) = FALSE` to look at only the intersection of the merge operation.

```{r}
nrow(elecData)
```

The merged data frame has 3112 rows, exactly one less row than the 2012 results. This explains the one row discrepancy noted above: one county appears in the 2012 data that is missing in the 2016 data. So, while we have shown that only the Alaska results would be dropped from the 2016 data frame, we must determine what county is dropped from the 2012 data frame.

```{r}
results2012[!(results2012$countyFIPS %in% elecData$countyFIPS),]
elecData[elecData$county == "Bedford",1:5]
```

Bedford City, Virginia is the missing row. However, Bedford, VA still appears in the merged results, and the city is a part of the county (https://en.wikipedia.org/wiki/Bedford,_Virginia). The county that is preserved in the data frame has a significantly larger number of votes than Bedford City, so the remaining county will be a sufficient analogue, particularly for mapping purposes.

## Geographic Information
#### Credit: Haeri (purely visual/name edits by Thorsen)

```{r}
gmlCoercer <- function(states) {
  ## Splits a messy string of county geographic information
  ## into three components: name, latitude, and longitude.
  string <- gsub("\n", "", states)
  string <- gsub("^[[:blank:]]+|[[:blank:]]+$", "", string)
  string <- gsub("(?<=[[:alpha:]])[[:blank:]]+|[[:digit:]][[:blank:]]+", " ", string, perl = TRUE)
return(strsplit(string, 
                split = "(?<=[[:alpha:]])([[:blank:]])(?=[[:punct:]]|[[:digit:]])|[[:digit:]][[:blank:]][[:digit:]]", 
                perl = TRUE))
}
```

```{r geographic results, eval = FALSE}

## Append DC code to default set of state abbreviations
stateCodes <- c(state.abb, "DC")

geoInfo <- lapply(stateCodes, function(abb) {
  
  ## Step 1:
  ## Get root of GML file
  gmlURL <- "http://www.stat.berkeley.edu/users/nolan/data/voteProject/counties.gml"
  gmlFile <- xmlParse(gmlURL)
  gmlRoot <- xmlRoot(gmlFile)
  
  ## Step 2:
  ## xPath for reaching the county level geographic information for each state
  countyPath = paste("//doc/state/gml:name[@abbreviation='",abb,"']/parent::state/county",sep="")

  ## Step 3a:
  ## Obtain and clean up messy geographic information string;
  ## result is list of vectors of length 3
  countyValues = gmlCoercer(xpathSApply(gmlRoot, countyPath, xmlValue))
  
  ## 3b: Combine information from sublists by index
  countyName = sapply(countyValues, function(elem) {elem[[1]]})
  lat = sapply(countyValues, function(elem) {elem[[2]]})
  long = sapply(countyValues, function(elem) {elem[[3]]})

  ## Step 4:
  ## Return information as data frame
  return(data.frame(state = rep(abb, length(countyPath)), 
                    county = countyName, 
                    latitude = as.numeric(lat), 
                    longitude = as.numeric(long)))
})

## Combine list elements into single data frame
geoInfo <- Reduce(rbind, geoInfo[-1], init = geoInfo[[1]])
```

```{r, echo = FALSE}
load("data/geoInfo.rda")
```

```{r}
## Coerce geoInfo columns into more useful data types
geoInfo$state <- as.character(geoInfo$state)
geoInfo$county <- as.character(geoInfo$county)
```

## Merging Geographic Information
#### Credit: Thorsen

It is probable that most county names are shared between `geoInfo` and the current merged table.

```{r}
mean(elecData$county_name %in% geoInfo$county)
```

Just as we expected. However, many county names are duplicates across states. To account for this, we will paste together the county name and state name as new variables in each data frame, and see how this affects the percentage for which an intersection merge will retain the data.

```{r}
elecData$countyState <- paste(elecData$county_name,", ",elecData$state_abbr, sep="")
geoInfo$countyState <- paste(geoInfo$county,", ",geoInfo$state, sep="")

mean(elecData$countyState %in% geoInfo$countyState)
```

Using these variables will result in over 99% of the data successfully being merged. However, let us examine the counties in each data set for which their name cannot be found in the other data frame.

```{r}
elecData[!(elecData$countyState %in% geoInfo$countyState),"countyState"]

geoInfo[!(geoInfo$countyState %in% elecData$countyState) 
        & geoInfo$state != "AK","countyState"]
```

By cross referencing the two tables, all but one of the counties in `results12and16` can be accounted for. LaSalle and McKean do not show up due to an added space in the `geoInfo` table. Oglala County, SD is due to a change in name: it was previously referred to as Shannon County until May 2015 (https://en.wikipedia.org/wiki/Oglala_Lakota_County,_South_Dakota).

Manual reassignment of these three names will allow for a successful merge. Broomfield County will unfortunately be lost in this merge operation.

```{r manual assign county names}
geoInfo[geoInfo$countyState == "La Salle County, IL","countyState"] <- "LaSalle County, IL"
geoInfo[geoInfo$countyState == "Mc Kean County, PA","countyState"] <- "McKean County, PA"
geoInfo[geoInfo$countyState == "Shannon County, SD","countyState"] <- "Oglala County, SD"
```

```{r merge geo}
elecData <- merge(x = elecData, y = geoInfo,
                           by.x = "countyState", by.y = "countyState",
                           all.x = FALSE, all.y = FALSE)

nrow(elecData)
```

Another intersection merge, which leaves the data thus far almost entirely intact. Only two counties out of 3113 have been involuntarily dropped in the merge operations so far -- not bad!

## 2010 Census Data
#### Credit: Saraie

This step of the project is to create a data frame that consists of data from six sources. Three sources contain election data, and the other three contain census data. In this segment of the project, I am putting data from Source #5 (2010 census) into the complete data frame.

First, I will load the data from the appropriate sources:

```{r}
DataB01 = read.csv("http://www.stat.berkeley.edu/users/nolan/data/voteProject/census2010/B01003.csv")

## Race information

DataDP02 = read.csv("http://www.stat.berkeley.edu/~nolan/data/voteProject/census2010/DP02.csv")

## Socio data

DataDP03 = read.csv("http://www.stat.berkeley.edu/~nolan/data/voteProject/census2010/DP03.csv")

## Economic data

## It's important to note here that all of the data that has just been downloaded is put out in the form of a data frame.
```

Now that all of the data is loaded, let's check to see if any of these data frames have missing values for any of the present variables. We will sapply each data frame to check for NA values at each row, and sum up all of the ones that are found.

```{r}
sum(sapply(DataB01, function(x) sum(is.na(x))))
sum(sapply(DataDP02, function(x) sum(is.na(x))))
sum(sapply(DataDP03, function(x) sum(is.na(x))))

```

All of them have an output of 0, so we don't have to worry about clearing NA values.

Now we can begin looking at the different variables that we want to incorporate into the data frame.We'll first take a look at HD01_VD01 column in the B01 data frame to make 2 columns of the total estimated population by each county and the estimated white population. The process is that we will take the columns of the county names, signifiers of race, and population values of each race; we make a separate data frame from those columns (titled CensusDF), which will be the main data frame for putting in information.

For reference, the variable names can be found at http://www.stat.berkeley.edu/~nolan/data/voteProject/census2010/B01_metadata.txt 

```{r}
sum(DataB01$POPGROUP.display.label == "Total population")

## This function gives us a good sense of how many counties we're dealing with in total. We have 3217 counties. We're setting the standard equal to total population because every county has a listed total population in the data frame, so finding the sum gives us the total number of counties.

CensusDF <- cbind.data.frame(DataB01$GEO.display.label, DataB01$POPGROUP.display.label, DataB01$HD01_VD01) 

## Create a DF of all the columns we need. In this case, we just need the county names, the population labels, and the estimated total population. 

colnames(CensusDF) <- c("County", "POPGROUP.display.label", "Estimated Population")

## Changing the column names to make data editing easier.

## At this point, we have a clean data frame that has all of the variables we want.

```

The issue with CensusDF at this point is that the data frame does not give us separate columns for the white population and for the total population. We do this by creating 2 separate data frames from CensusDF for the total population and the white population; we then combine the population number columns from each to make a data frame that has the county, the white population, and the total population.

```{r}
CensusDFtotal <- CensusDF[CensusDF$POPGROUP.display.label == "Total population", ]
CensusDFwhite <- CensusDF[CensusDF$POPGROUP.display.label == "White alone", ]

## Subsetting the data frames to only give values that display the total population of each county and the white population, which is what we want. 

CensusDFtotal$POPGROUP.display.label <- NULL
CensusDFwhite$POPGROUP.display.label <- NULL

## We remove the POPGROUP.display.label column, as we do not need that information anymore to maintain the clarity of the new data frame.

colnames(CensusDFwhite) <- c("County", "White Population")
colnames(CensusDFtotal) <- c("County", "Total Population")

## We change the column names here to make the data more clear.

CensusDF <- merge(CensusDFwhite, CensusDFtotal, by = "County", all.x = TRUE, all.y = TRUE)

## Merging the two data frames into one.
```

Now we have two columns done, with the estimated total population and the estimated white population. 
We make our next few variable observations by merging columns from the B01, DP02, and DP03 data frames into CensusDF, which will give us a data frame with about 30-40 variables.

Adding several extra columns from the DP02 table, representing different variables by county. We are specifically observing these variables from the 2010 census:

- HC01_VC03 - Estimated # of households
- HC03_VC04 - Percentage of family households (families)
- HC03_VC06 - Percentage of family households (families) with children under 18 years
- HC03_VC07 - Percentage of households with a married-couple family
- HC03_VC08 - Percentage of married families with children under 18 years
- HC03_VC09 - Percentage of families with a male householder and no wife
- HC03_VC11 - Percentage of families with a female householder and no husband
- HC03_VC13 - Percentage of nonfamily households
- HC03_VC14 - Percentage of households with a lone householder
- HC03_VC15 - Percentage of households with a lone householder at least 65 years old
- HC03_VC17 - Percentage of households with one or more people under 18 years
- HC03_VC18 - Percentage of households with one or more people over 65 years
- HC01_VC20 - Estimated # of average household size
- HC01_VC21 - Estimated # of average family size
- HC03_VC36 - Percentage of people never married
- HC03_VC37 - Percentage of people now married, except separated
- HC03_VC38 - Percentage of people separated
- HC03_VC94 - Percentage of people with bachelors or higher

For reference, the variable names can be found at
http://www.stat.berkeley.edu/~nolan/data/voteProject/census2010/DP02_metadata.txt

```{r}
CensusDF <- merge(CensusDF, DataDP02[ ,c("GEO.display.label", "HC01_VC03", "HC03_VC04","HC03_VC06", "HC03_VC07","HC03_VC08", "HC03_VC09", "HC03_VC11", "HC03_VC13", "HC03_VC14", "HC03_VC15", "HC03_VC17", "HC03_VC18", "HC01_VC20", "HC01_VC21", "HC03_VC36", "HC03_VC37", "HC03_VC38", "HC03_VC94")], by.x = "County", by.y = "GEO.display.label", all.x = TRUE, all.y = TRUE)

## We merge CensusDF with all of the relevant DP02 columns, choosing to accept NA values that may be present to allow the data frames to merge.
```

Adding several extra columns from the DP03 table, representing different variables by county. We are specifically observing these variables from the 2010 census:

- HC01_VC04 - Estimated # of population 16 years and over with an employment status
- HC03_VC05 - Percentage of population in the labor force
- HC01_VC06 - Estimated # of population in the civilian labor force
- HC03_VC07 - Percentage of people employed in the civilian labor force
- HC03_VC08 - Percentage of people unemployed in the civilian labor force
- HC01_VC10 - Estimated # of population not in the labor force
- HC03_VC13 - Percentage of population unemployed
- HC03_VC16 - Percentage of females 16 and over in the labor force
- HC01_VC17 - Estimated # of females 16 and over in the civilian labor force
- HC03_VC18 - Percentage of females 16 and over employed in the civilian labor force
- HC01_VC31 - Estimated # of people commuting to work by public transit (excluding taxi)
- HC01_VC34 - Estimated # of people working at home
- HC01_VC42 - Estimated # of employed civilians with service occupations
- HC01_VC51 - Estimated # of employed civilians in construction
- HC01_VC52 - Estiamted # of employed civilians in manufacturing
- HC01_VC54 - Estimated # of employed civilians in retail trade
- HC01_VC85 - Estimated amount of median household income

For reference, the variable names can be found at
http://www.stat.berkeley.edu/~nolan/data/voteProject/census2010/DP03_metadata.txt

```{r}
CensusDF <- merge(CensusDF, DataDP03[ ,c("GEO.display.label", "HC01_VC04", "HC03_VC05", "HC01_VC06", "HC03_VC07", "HC03_VC08", "HC01_VC10", "HC03_VC13", "HC03_VC16", "HC01_VC17", "HC03_VC18", "HC01_VC31", "HC01_VC34", "HC01_VC42", "HC01_VC51", "HC01_VC52", "HC01_VC54", "HC01_VC85")], by.x = "County", by.y = "GEO.display.label", all.x = TRUE, all.y = TRUE)

## Same process as with the DataDP02 data frame, but with the DP03 data.
```

Now that the entire data frame has all of the data we want, we can begin changing the column names to give some context to each column. We use the colnames function:

```{r}
colnames(CensusDF) <- c("County", "whitePop", "totalPop", 
                        "households", "hhFamily", "hhFamilyChildren",
                        "hhMarried", "famWithChildren", "famSingleDad", "famSingleMom", 
                        "hhNonFamily", "hhLoneHolder", "hhLoneSenior", "hhChildren",
                        "hhSenior", "hhAvgSize", "famAvgSize", 
                        "popSingle", "popMarriedSeparated", "popSeparated", "popBachelors",
                        "popOver16", "popLaborForce", "civilianLF", "employedCLF", "unemployedCLF",
                        "popNotLaborForce", "unemployed",
                        "femLaborForce", "femCLF", "femEmployedCLF",
                        "publicTransit", "workAtHome",
                        "employedServiceOccupations", "construction", "manufacturing", "retail",
                        "medianIncome")
```

## Merging Census Data
#### Credit: Saraie

I now begin the process of merging my data from CensusDF into the larger dataframe. To do this, I separate the state from the county in CensusDF$County, edit the names of the state in CensusDF so that they match up with the main data frame, "results12and16", and then merge the two data frames such that I get all of the info on one table.

```{r}
## Locale must be specified for strsplit; Not sure why this works
## but recommended on https://victorfang.wordpress.com/2015/03/26/r-locale-error-super-weird/
Sys.setlocale(locale="C")

CensusDF$County <- as.character(CensusDF$County)
list <- strsplit(CensusDF$County, ", ")

## We change CensusDF$County to a character class so we can use strsplit to divide the column by county and state. The presence of a comma in each row makes this process very simple.

df <- ldply(list)

## We use plyr to make a data frame from our list, which we named "list". The substitute data frame is named "df".

CensusDF <- cbind(df, CensusDF)

## Combining the data frames.

CensusDF$County <- NULL

## Removing the original "County" column, which we do not need anymore.

names(CensusDF)[names(CensusDF) == 'V1'] <- 'County'
names(CensusDF)[names(CensusDF) == 'V2'] <- 'State'

## Renaming the column names to make data more meaningful.

CensusDF$State <- tolower(CensusDF$State)

## Making the entire state column lowercase. The state column in results12and16 is lowercase, so we do this to make the final merge possible.

CensusDF$State <- gsub(" ", "-", CensusDF$State)

## Replacing every space with a dash. As before, we're just making the state columns from the difference data frames identical.

elecDataCensus <- merge(elecData, CensusDF, 
                        by.x = c("county_name", "state.x"), by.y = c("County", "State"), 
                        all.x = FALSE, all.y = FALSE)

## The final merge.
```

```{r}
nrow(elecDataCensus) - nrow(elecData)
```

5 counties were lost in performing a merge on the intersection. This is not a number to be hugely concerned with, but it is worth seeing if we can preserve any of them that were lost due to nomenclature/string representation issues.

```{r}
elecData[!(elecData$county_name %in% CensusDF$County),"countyState"]
```

Oglala County is likely lost in the merge due to the reassignment used in the merging of geographic information. Kenedy County is not a major concern, as it is the fourth least populous county in the US . Loving County is also the least populous county in the US. These small data points are not a major concern, but Dona Ana County is the second largest county in New Mexico, which makes this county loss rather troubling.

- https://en.wikipedia.org/wiki/Kenedy_County,_Texas
- https://en.wikipedia.org/wiki/Loving_County,_Texas
- https://en.wikipedia.org/wiki/Doña_Ana_County,_New_Mexico

I have now successfully merged all of my data to the master data frame! We can now simplify our environment by removing intermediate results.

```{r}
elecData <- elecDataCensus
rm(elecDataCensus)
```

## 2008 Results
#### Credit: Saraie, Marroquin

In this section of merging all of the data into the data frame, we are now looking at the 2008 election data. Bear in mind, the data can be accessed from two separate sources:

- A google sheets page available online at https://docs.google.com/spreadsheets/d/1gLzjUFBk9gtAPfZ-bNZVfFC1zNhGkY_WI_VD_OXHUYI/edit#gid=0

- A downloadable excel file available at
http://www.stat.berkeley.edu/users/nolan/data/voteProject/countyVotes2008.xlsx

To begin this entire process, we install the "readxl" R package and write a function that will allow us to store multiple excel sheets into one list.

```{r}
## At this point, "readxl" has been manually installed.

library(readxl) 

## The standard requiring of the package.
## The website https://blog.rstudio.org/2015/04/15/readxl-0-1-0/ was very useful in finding the right package to read the Excel data.

LoadExcelSheets <- function(filename) {
    sheets <- readxl::excel_sheets(filename)
    x <- lapply(sheets, function(X) readxl::read_excel(filename,         sheet = X))
    names(x) <- sheets
    x
}

## This is an important function. It takes the file name and sets all of the sheets into a list, as seen by the lapply reading every excel sheet file into the variable "x". 

## After this function, it is important to note that I have downloaded the excel file into my computer, so when I want to read it from the function, I am able to.

## The website http://stackoverflow.com/questions/12945687/read-all-worksheets-in-an-excel-workbook-into-an-r-list-with-data-frames provided extremely useful information in storing multiiple sheets to a single list.

Election08 <- LoadExcelSheets("data/countyvotes2008.xlsx")

## We save the entire excel file into the R environment.
```

The next few steps are simply cleaning the data to be useful for the final merge. 

```{r}
Election08$`Total results`<- NULL

## We get rid of the "Total Results" sheet from the list, as the data is not useful on a specific county basis.

Election08$Alaska <- NULL

## There is little to no Alaska data in this entire project, so we get rid of the Alaska data.

Election08 <- lapply(Election08, function(x) { x[[7]] <- NULL; x })

## This function is useful in that it gets rid of the seventh column in every data frame of the list; all of the seventh columns were blank, and only contained NA values. 

Election08 <- lapply(Election08, as.data.frame)

## For the sake of simplicity, every data frame in the list is confirmed in its class as a data frame.
```

Upon looking at the list, all of the counties have an extra space after their name. We begin the process of fixing that issue and refitting the data.

```{r}
Election08 <- lapply(Election08, function(d) {
  as.data.frame(lapply(d, gsub, pattern = "[[:space:]](*$)", replacement = "")
  )} )

## This webpage (http://stackoverflow.com/questions/26853287/how-to-use-gsub-to-remove-parts-from-a-list-in-r) was quite helpful in figuring out how to change parts of a list in R.

## We want to get rid of every data entry with a space at the end of the county name, as that makes the list more compatible with the county names from results12and16.

colnames <- c("County", "Total Precincts", "Precincts Reporting", "Obama", "McCain", "Other")
Election08 <- lapply(Election08, setNames, colnames)

## Since the names of the columns changed when we used the lapply function, we set the names back to their original titles using a new lapply call.

Election08 <- lapply(Election08, function (elem) {
  elem$County <- as.character(elem$County)
  elem$'Total Precincts' <- numCoercer(elem$'Total Precincts')
  elem$'Precincts Reporting' <- numCoercer(elem$'Precincts Reporting')
  elem$Obama <- numCoercer(elem$Obama)
  elem$McCain <- numCoercer(elem$McCain)
  elem$Other <- numCoercer(elem$Other)
  return(elem)
})

## The first lapply function changed all of the data to a factor class. As represented in this function, the County column reverts back to its character class, and the other columns revert back to their numeric class. 

## Using lapply has been quite useful thus far, and allows us to manipulate each element of the list quite efficiently.

## We use the numCoercer function previously established from the 2012 and 2016 data to get the correct values from the columns with numeric values.

```

The data has now been cleaned, and is ready for merging! The next few codes chunk list the specific calls used to put the election data into results12and16. We begin by taking the list of state names from the 2012 election data, since those states are exactly the same as the ones from 2008. We clear out the Alaska and Washington DC data, as our Election08 list does not contain significant values from either of those regions.

The next step is making a function that adds a new column to each data frame in the list with the respective state of each county. We do this by taking our filtered state names and placing them to each respective state in the list.

```{r}
stateNames <- read.csv("http://www.stat.berkeley.edu/users/nolan/data/voteProject/countyVotes2012/stateNames.txt")

## Reading the csv file.

stateNames <- as.character(stateNames$states)
stateNamesFiltered <- stateNames[stateNames != "alaska" & stateNames != "district-of-columbia"]

## Cleaning the data.

Election08 <- lapply(c(1:49), function(i) {
  stateDF <- Election08[[i]]
  stateDF$stateName <- rep(stateNamesFiltered[i], times = nrow(stateDF))
  return(stateDF)
})

## The function itself. Adds the states from the 2012 file.
```

The actual merging process can begin.

```{r}

## The plyr package is useful for merging a list of data frames into a single data frame. We do this so we can merge this data frame to results12and16.

Election08 <- ldply(Election08, data.frame)

## We learned how to use the ldply function from http://stackoverflow.com/questions/2851327/convert-a-list-of-data-frames-into-one-data-frame

elecData <- merge(elecData, Election08, by.x = c("state.x", "county.x"), by.y = c("stateName", "County"), all.x = FALSE, all.y = FALSE)

## Merging the 2008 election data with elecData.

```

## 2004 Results
#### Credit: Amidi-Abraham

To begin analysis of the 2004 election data, the variables will be altered and organized for ease of access in future steps.

```{r}
# Read data into R
pres_2004 = read.table(file = 'http://www.stat.berkeley.edu/users/nolan/data/voteProject/countyVotes2004.txt', header = TRUE)
# Convert variable factors to character and numeric
pres_2004$countyName = as.character(pres_2004$countyName)
pres_2004$bushVote = as.numeric(pres_2004$bushVote)
pres_2004$kerryVote = as.numeric(pres_2004$kerryVote)
# Split "countyName" variable into "stateName" and "countyName" variables
pres_2004$stateName = sapply(strsplit(pres_2004$countyName, split = ','), "[[", 1)
pres_2004$countyName = sapply(strsplit(pres_2004$countyName, split = ','), "[[", 2)
# Alphabetize states
pres_2004 = pres_2004[order(pres_2004$stateName), ]
```

After counting the number of states present in the data, it is quick to notice the lack of certian states, notably Alaska, Hawaii, and Virginia

```{r}
state_names = read.table(url('http://www.stat.berkeley.edu/users/nolan/data/voteProject/countyVotes2012/stateNames.txt'), header = TRUE)
state_names = as.character(state_names$states)
state_names = gsub('-', ' ', state_names)

setdiff(state_names, unique(pres_2004$stateName))
```

We will extract the Virginia data from Wikipedia, as introduced in lecture. Virginia is a particular case where many cities are independent of the counties they are located in. Wikipedia mentions "County or City" when labeling. Some of cities are renamed to better match the master data frame.

```{r}
# Extracting Virginia Data from Wikipedia (Lecture 35, pgs. 11-14)
library(XML)
library(RCurl)
# Identify URL
wikiURL = "https://en.wikipedia.org/wiki/United_States_presidential_election_in_Virginia,_2004"
# Set up HTML Table
pageContents = getURLContent(wikiURL)
pDoc = htmlParse(pageContents)
pRoot = xmlRoot(pDoc)
cTable = getNodeSet(pRoot,"//table/tr/td/a[@title='Accomack County, Virginia']/../../..")

tableChar =
  do.call(rbind, sapply(1:xmlSize(cTable[[1]]),
                        function(i) {strsplit(xmlValue(cTable[[1]][[i]]), "\n")
                        }))

vals04 =
  apply(tableChar[-1, -1], 2,
        function(vec) {
          as.numeric(gsub("[%,]", "", vec))
        })

wikiTables = readHTMLTable(pageContents)
sapply(wikiTables, dim)

countyDF = readHTMLTable(pageContents, which = 9, colClasses =
                           c("character",
                             "Percent","FormattedInteger",
                             "Percent","FormattedInteger",
                             "Percent","FormattedInteger"),
                         stringsAsFactors = FALSE)
# Clean and format Virignia data before merging with rest of country
countyDF = countyDF[ ,c(1,3,5)]
countyDF$stateName = "virginia"
colnames(countyDF) <- c("countyName", "kerryVote", "bushVote", "stateName")
countyDF$countyName = sapply(strsplit(countyDF$countyName, split = ','), "[[", 1)
countyDF$countyName[countyDF$countyName == "Fairfax"] = "Fairfax City"
countyDF$countyName[countyDF$countyName == "Franklin"] = "Franklin City"
countyDF$countyName[countyDF$countyName == "Richmond"] = "Richmond City"
countyDF$countyName[countyDF$countyName == "Roanoke"] = "Roanoke City"
```

Repeat process for Hawaii. The "Overseas" data from Wikipedia is removed since it is not contained in any of the other election years.

```{r}
# Extracting Hawaii Data from Wikipedia
# Identify URL
wikiURL = "https://en.wikipedia.org/wiki/United_States_presidential_election_in_Hawaii,_2004"
# Set up HTML Table
pageContents = getURLContent(wikiURL)
pDoc = htmlParse(pageContents)
pRoot = xmlRoot(pDoc)
cTable = getNodeSet(pRoot,"//table/tr/td/a[@title='Hawaii County, Hawaii']/../../..")

tableChar =
  do.call(rbind, sapply(1:xmlSize(cTable[[1]]),
                        function(i) {strsplit(xmlValue(cTable[[1]][[i]]), "\n")
                        }))

vals04 =
  apply(tableChar[-1, -1], 2,
        function(vec) {
          as.numeric(gsub("[%,]", "", vec))
        })

wikiTables = readHTMLTable(pageContents)
sapply(wikiTables, dim)

hawaiiDF = readHTMLTable(pageContents, which = 11, colClasses =
                           c("character",
                             "Percent","FormattedInteger",
                             "Percent","FormattedInteger",
                             "Percent","FormattedInteger"),
                         stringsAsFactors = FALSE)
# Clean and format Virignia data before merging with rest of country
hawaiiDF = hawaiiDF[c(2:5) ,c(1,3,5)]
hawaiiDF$stateName = "hawaii"
colnames(hawaiiDF) <- c("countyName", "kerryVote", "bushVote", "stateName")
# Combine Hawaii data with Virginia dataframe
countyDF = rbind.data.frame(hawaiiDF, countyDF)
```

Virginia and Hawaii are combined with the rest of the United States to form a set with 49 states plus Washington D.C.

```{r}
# Combine Virginia and Hawaii data with rest of country
pres_2004 = rbind.data.frame(pres_2004, countyDF)
# Capitalize county names
pres_2004$countyName = gsub("(^|[[:space:]])([[:alpha:]])", "\\1\\U\\2", pres_2004$countyName, perl=TRUE)
# Alphabetize states
pres_2004 = pres_2004[order(pres_2004$stateName), ]
```


## Merging 2004 Results
#### Credit: Thorsen

```{r}
head(pres_2004[,c("countyName", "stateName")])
head(elecData[,c("county.x", "state.x")])
```

These two pairs of columns seem like reasonable choices for the merge operation; the counties are both capitalized and state names remain in lower case; a simple paste call can combine the two so we may see how many counties in `elecData` are also found in `pres_2004`.

```{r}
elecData$merge2004Info <- paste(elecData$county.x, elecData$state.x, sep = ", ")
pres_2004$mergeInfo <- paste(pres_2004$countyName, pres_2004$stateName, sep = ", ")

mean(elecData$merge2004Info %in% pres_2004$mergeInfo)
```

Evidently, this is not hugely successful as is. Let's see if some simple fixes, particularly regarding state names, can retain more county information.

```{r}
unique(elecData[!(elecData$state.x %in% pres_2004$stateName),]$state.x)
```

States with two components to their names in elecData are represented with hyphens, so a `gsub` on the merge information column should fix this.

```{r}
elecData$merge2004Info <- gsub("-", " ", elecData$merge2004Info)

mean(elecData$merge2004Info %in% pres_2004$mergeInfo)
```

Much better, but not perfect. Let's see what counties are still missing.

```{r}
elecData[!(elecData$merge2004Info %in% pres_2004$mergeInfo),]$merge2004Info
```

Several important counties would be missing 2004 election results if we merge on these columns-- St. Louis, Richmond, Brooklyn, and Manhattan among others. While manual reassignment could account for some of these, but retaining the 2004 results is less important than later elections for predicting the outcome for 2016. As such, we will omit the 2004 information for these 99 counties, but keep them in the data frame. This is accomplished by specifying `all.x = TRUE` and `all.y = FALSE` in the merge operation, to perform a left join.

```{r}
preNumCounties <- nrow(elecData)

elecData <- merge(x = elecData, y = pres_2004,
                  by.x = "merge2004Info", by.y = "mergeInfo",
                  all.x = TRUE, all.y = FALSE)

preNumCounties == nrow(elecData)
c(preNumCounties, nrow(elecData))
```

Five additional rows cropped up during this merge operation. Likely, this is because there are duplicate strings (identical counties) in the merge information column used in the data frame. We will have to eliminate these in the final cleaning up of the data.


## Cleaning Up Results
#### Credit: Thorsen

Along the process of merging the data sources, a large amount of extraneous and redundant columns have slowly amassed, primarily in terms of state and county information. Additionally, naming conventions across data sources vary greatly.

```{r}
names(elecData)
```

To simplify operations in the second half of this project, it will be useful to tidy up our primary data frame. This will be accomplished by (a) dropping identical variables, and (b) renaming the columns of our data frame as needed.

```{r}
## Names are copied from code used in obtaining census info
censusCols <- c("whitePop", "totalPop", 
                "households", "hhFamily", "hhFamilyChildren",
                "hhMarried", "famWithChildren", "famSingleDad", "famSingleMom", 
                "hhNonFamily", "hhLoneHolder", "hhLoneSenior", "hhChildren",
                "hhSenior", "hhAvgSize", "famAvgSize", 
                "popSingle", "popMarriedSeparated", "popSeparated", "popBachelors",
                "popOver16", "popLaborForce", "civilianLF", "employedCLF", "unemployedCLF",
                "popNotLaborForce", "unemployed",
                "femLaborForce", "femCLF", "femEmployedCLF",
                "publicTransit", "workAtHome",
                "employedServiceOccupations", "construction", "manufacturing", "retail",
                "medianIncome")

elecData <- elecData[,c("county_name", "state_abbr", 
                         "votes_dem", "votes_gop", ## 2016 results
                         "obamaVotes2012", "romneyVotes2012", ## 2012 results
                         "Obama", "McCain", ## 2008 results
                         "kerryVote", "bushVote", ## 2004 results
                         censusCols)]

## Reassign names for simplicity
names(elecData) <- c("county", "state",
                     "dem16", "gop16",
                     "dem12", "gop12",
                     "dem08", "gop08",
                     "dem04", "gop04",
                     censusCols)

head(elecData[,c(1:10)])
dim(elecData)
```

`elecData` is nearly finished, but the issue of the duplicate counties remains. A custom function must be written to address these discrepancies, which will single out the counties which appear multiple times in the data frame. To do so, we will look at each unique county-state value pair and count the number of times it appears in the data frame. If it appears more than once, we retain that county and return it. The function appears below.

```{r}
findDupes <- function(vec1, vec2) {
  vals <- paste(vec1, vec2, sep = ", ")
  uniqVals <- unique(vals)
  count <- sapply(uniqVals, function (val) {
    sum(vals == val)
  })
  return(uniqVals[count > 1])
}
```

```{r}
findDupes(elecData$county, elecData$state)
```

All of the duplicate counties appear in Virginia-- this is not surprising, as the Virginia results for 2004 were parsed off of Wikipedia. Additionally, there are exactly 5 cities which appear multiple times in the data frame, so the only time that duplicates formed was in the merging of the 2004 voting results, when 5 additional rows appeared during the left join operation.

```{r}
elecData[elecData$county %in% c("Bedford County", "Fairfax city", 
                                "Franklin city", "Richmond city", 
                                "Roanoke city")
         & elecData$state == "VA",
         c("county", "state", "dem04", "gop04", "totalPop")]
```

Looking at the 2004 results for the duplicate counties shows some obvious incongruities. The vote count for one year for each county is either absurdly low or greater than the population of the county. Selective filtering by index will remove the rows which contain these incongruous and duplicate results. Index numbers are obtained through manual inspection.

```{r}
elecData <- elecData[-c(166, 888, 947, 2394),]

elecData[elecData$county %in% c("Bedford County", "Fairfax city", 
                                "Franklin city", "Richmond city", 
                                "Roanoke city")
         & elecData$state == "VA",
         c("county", "state", "dem04", "gop04", "totalPop")]
```

It is not a simple matter to determine which row is incorrect for Roanoke City. To do so, we will compare the 2004 results to the results for other years.

```{r}
elecData[elecData$county == "Roanoke city" & elecData$state == "VA", c(1:10)]
```

Both outcomes seem reasonable. Rather than risk representing the data incorrectly, we will instead drop one of the rows from the data frame and set the 2004 election results for the other to `NA`.

```{r}
elecData <- elecData[!(elecData$county == "Roanoke city" & elecData$gop04 == 30596),]
elecData[elecData$county == "Roanoke city" & elecData$state == "VA",
         c("dem04", "gop04")] <- NA

elecData[elecData$county == "Roanoke city" & elecData$state == "VA", c(1:10)]
```

At this point, the data frame appears to be complete! Let us look at the information contained in the data frame to verify it seems reasonable.

```{r}
dim(elecData)

## Confirm that no duplicate rows remain in the data
length(findDupes(elecData$county, elecData$state)) == 0

## View what states are represented in data
sort(unique(elecData$state))

"DC" %in% elecData$state
```

The District of Columbia has been dropped from the data, but this is only one county out of over 3,000. We are not interested in calculating electoral results, but instead focusing on county level results, so a single data point does not greatly affect our progress later on.

Let us examine how many observations are missing for the entire data frame.

```{r}
missingCount <- sum(sapply(elecData, function(x) {sum(is.na(x))}))
missingCount
missingCount / prod(dim(elecData)) ## Rows * columns
```

Slightly more than 0.1% of the data is missing for the counties we retained in the data frame -- not bad at all!

For some simple verifications of results, let's examine the national popular vote results for each election year to confirm that the data seem reasonable.

```{r}
sapply(elecData[c(3:10)], sum, na.rm = TRUE) / (10 ^ 6) 
## Sum of popular vote in millions, by party, by year
```

The total votes in this data frame appear to be reasonable; for instance, Clinton holds a slight lead over Trump for the 2016 data, and in all other outcomes the winning party also won the popular vote.

#### Saving the results

At this point, we will save the results of our data frame to an .Rda file for later usage. 

```{r, eval = FALSE}
save(elecData, file = "elecData.rda")
```

This file is included in our submission, along with intermediate files used to simplify the knitting process. All code needed to recreate all such files is provided in this .Rmd file, but some code chunks are set to simply display their code rather than evaluate. For these code chunks, a hidden code chunk appears below in which an intermediate .Rda file is loaded.

## Conclusion

In preparing our data for later analysis, several issues arose which had to be addressed. Among these were the proper handling of data being represented in the incorrect types, such as numeric data appearing as strings with added characters. Some states were missing from the initial data frames provided: namely, Alaska was not present in the 2012 data, and Virginia and Hawaii were not present in the 2004 data. For the latter two, we were able to obtain the results through parsing data sources available elsewhere on the internet.

A particularly critical recurring issue was the handling of merges across the different data sources. To merge by county and state pairs, it was necessary to obtain such information from every data source, which was often represented in different ways. In one instance, the use of FIPS codes enabled seamless merging of the data, whereas other merges required string manipulations to facilitate the matching of values. Additionally, an issue arose in the matter of missing values and values which did not have a directly translatable equivalent in the data frame to be merged. To address these concerns, we examined which values would be lost in the merge along the intersection of both data frames, and assessed the possibility of manual reassignment of names. For counties in which manual reassignment was not a simple task, the next step was to determine how important the counties were, qualitatively speaking. For instance, it was a much easier decision to drop the least populous counties in New Mexico from the results than to drop Manhattan or Brooklyn, NY from the results.

Depending on the severity of information loss, merging operations occurred in one of two ways. If the data to be lost did not seem of high importance (e.g., low population size), an intersection merge was performed to preserve complete data and avoid the cropping up of `NA` values. If the counties were more important, it was preferable to preserve some information for these counties in a left join merge, and at least retain most of the data on such counties. This is admittedly a qualitative and subjective process, but our criteria is spelled out in all cases to allow the reader to view the process we chose.

Ultimately, we were able to obtain nearly complete data for 3,104 counties in the United States. Our largest single data source, the `CensusDF`, contains 3,217 rows, which suggests that on the whole, we retained a vast majority of the counties. This will enable us to obtain robust, meaningful conclusions in our later analysis of the 2016 election results.



#### Acknowledgements 

- Data hosting courtesy of Professor Deborah Nolan, University of California Berkeley, Department of Statistics

- 2016 results originally available from https://github.com/tonmcg/County_Level_Election_Results_12-
16/blob/master/2016_US_County_Level_Presidential_Results.csv

- 2012 results originally available from *Politico*: http://www.politico.com/2012-election/map/#/President/2012/

- 2008 results originally available from *The Guardian*: https://www.theguardian.com/news/datablog/2009/mar/02/us-elections-2008

- 2004 results available from Deborah Nolan (UC Berkeley): http://www.stat.berkeley.edu/users/nolan/data/voteProject/countyVotes2004.txt

- 2010 Census data available from http://factfinder2.census.gov/faces/nav/jsf/pages/searchresults.xhtml?refresh=t

- Geographic information available from Deborah Nolan: http://www.stat.berkeley.edu/users/nolan/data/voteProject/counties.gml

- `numCoercer` utility function based on http://stackoverflow.com/questions/1523126/how-to-read-data-when-some-numbers-contain-commas-as-thousand-separator

- R Core Team (2016). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.

- Duncan Temple Lang and the CRAN team (2016). RCurl: General Network (HTTP/FTP/...) Client Interface for R. R package version 1.95-4.8. https://CRAN.R-project.org/package=RCurl

- Duncan Temple Lang and the CRAN Team (2016). XML: Tools for Parsing and Generating XML Within R and S-Plus. R package version 3.98-1.4. https://CRAN.R-project.org/package=XML

- Hadley Wickham (2011). The Split-Apply-Combine Strategy for Data Analysis. Journal of Statistical Software, 40(1), 1-29. URL http://www.jstatsoft.org/v40/i01/.

- Hadley Wickham (2016). readxl: Read Excel Files. R package version 0.1.1. https://CRAN.R-project.org/package=readxl

#### Session information

```{r, echo = FALSE}
sessionInfo()
```
